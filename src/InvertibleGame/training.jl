import Flux
using Random

import ..TrainingAPI: build, train!, gradient

_unpack_flow_sample(sample) = begin
    sample isa NamedTuple || throw(ArgumentError("each dataset element must be a NamedTuple"))
    (haskey(sample, :context) && haskey(sample, :sample)) ||
        throw(ArgumentError("each dataset element must have keys `:context` and `:sample`"))
    return sample.context, sample.sample
end

_as_vec(x) = x isa AbstractVector ? x : vec(x)

_ema_beta(step::Integer, beta_start::Real, beta_final::Real, tau::Real)::Float32 = begin
    β0 = Float32(beta_start)
    βf = Float32(beta_final)
    τ = Float32(tau)
    (0f0 <= β0 <= 1f0) || throw(ArgumentError("ema_beta_start must be in [0, 1]; got $beta_start"))
    (0f0 <= βf <= 1f0) || throw(ArgumentError("ema_beta_final must be in [0, 1]; got $beta_final"))
    isfinite(τ) && (τ > 0f0) || throw(ArgumentError("ema_tau must be finite and > 0; got $tau"))
    t = Float32(step)
    return βf - (βf - β0) * exp(-t / τ)
end

_ema_update!(ema_model, live_model, β::Float32) = begin
    ema_ps = Flux.trainables(ema_model)
    live_ps = Flux.trainables(live_model)
    length(ema_ps) == length(live_ps) || throw(ArgumentError("EMA model structure mismatch"))
    α = 1f0 - β
    for i in eachindex(ema_ps)
        ema_ps[i] .= β .* ema_ps[i] .+ α .* live_ps[i]
    end
    return ema_model
end

_update_memory(x_mem::Matrix{Float32},
               c_mem::Matrix{Float32},
               batch_size::Int,
               x_cand::Matrix{Float32},
               c_cand::Matrix{Float32},
               true_norms::AbstractVector{<:Real}) = begin
    size(x_cand, 2) == size(c_cand, 2) ||
        throw(DimensionMismatch("candidate context batch must match candidate sample batch"))
    length(true_norms) == size(x_cand, 2) ||
        throw(DimensionMismatch("true_norms length must match candidate batch"))

    v = Float32.(max.(0, true_norms .- 1))
    idx = findall(>(0f0), v)
    if isempty(idx)
        return (zeros(Float32, size(x_mem, 1), 0), zeros(Float32, size(c_mem, 1), 0), false)
    end

    k = min(batch_size, length(idx))
    keep_local = partialsortperm(view(v, idx), 1:k; rev=true)
    keep = idx[keep_local]
    return (x_cand[:, keep], c_cand[:, keep], true)
end

"""
    train!(model, data_iter; kwargs...) -> NamedTuple

Self-adversarial training for a single [`InvertibleCoupling`](@ref), using an EMA copy to provide fake samples.

The live model is trained to:
- include true samples (pull inside its own latent ball), and
- reject fake samples produced by the EMA model (push outside its own latent ball).

The fake samples are generated by decoding latents sampled from the chosen norm ball (same sampler used elsewhere).

# Arguments
- `model`: [`InvertibleCoupling`](@ref), updated in-place.
- `data_iter`: iterable dataset of named tuples `(; context, sample)`.

# Keyword Arguments
- `epochs=1`: number of dataset passes. `epochs=0` performs no updates (but still loads/saves if enabled).
- `batch_size=32`: batch size used when `data_iter` yields single samples (vectors).
- `margin_true=0.5`: margin passed to the true inclusion hinge.
- `margin_adv=0.0`: margin passed to the reject hinge (fake rejection).
- `norm_kind=:l1`: norm used in hinge losses (`:l1`, `:l2`, or `:linf`).
- `w_true=1.0`: weight on the true inclusion loss.
- `w_reject=1.0`: weight on the fake rejection loss.
- `latent_radius_min=0.0`: minimum radius in `[0, 1]` for sampled fake latents.
- `ema_beta_start=0.0`: initial EMA decay.
- `ema_beta_final=0.9999`: final EMA decay.
- `ema_tau=10000.0`: EMA schedule time constant (in optimizer steps).
- `use_memory=false`: optional hard-sample memory (variable size).
- `opt=Flux.Adam(1f-3)`: optimiser rule used for the model.
- `rng=Random.default_rng()`: RNG passed to [`gradient`](@ref) to sample latents for fake generation.
- `save_path=""`: checkpoint path; empty disables saving.
- `load_path=save_path`: checkpoint path to load from if it exists; empty disables loading.
- `save_period=60.0`: minimum time (seconds) between periodic saves.

# Returns
Named tuple `(; model, ema, losses)` where:
- `ema`: EMA copy used as opponent.
- `losses::Vector{Float32}`: per-update loss trace for `model`.
"""
	function train!(model::InvertibleCoupling,
	                data_iter;
	                epochs::Integer=1,
	                batch_size::Integer=32,
	                margin_true::Real=0.5,
	                margin_adv::Real=0.0,
	                norm_kind::Symbol=:l1,
	                w_true::Real=1.0,
	                w_reject::Real=1.0,
	                grad_mode::Symbol=:sum,
	                ema_beta_start::Real=0.0,
	                ema_beta_final::Real=0.9999,
	                ema_tau::Real=10000.0,
	                latent_radius_min::Real=0.0,
	                use_memory::Bool=false,
	                opt=Flux.Adam(1f-3),
	                rng::Random.AbstractRNG=Random.default_rng(),
	                save_path::AbstractString="",
	                load_path::AbstractString=save_path,
                save_period::Real=60.0)
    epochs >= 0 || throw(ArgumentError("epochs must be non-negative"))
    batch_size > 0 || throw(ArgumentError("batch_size must be positive"))

    save_path_final = String(save_path)
    load_path_final = String(load_path)
    save_enabled = !isempty(save_path_final)
    load_enabled = !isempty(load_path_final)
	    save_period = Float64(save_period)
	    save_period >= 0 || throw(ArgumentError("save_period must be non-negative"))
	    (grad_mode === :sum || grad_mode === :orthogonal_adv) ||
	        throw(ArgumentError("unsupported grad_mode=$(repr(grad_mode)); expected :sum or :orthogonal_adv"))

    if save_enabled
        mkpath(dirname(save_path_final))
    end

	    losses_loaded = nothing
	    ema = nothing
	    ema_beta_start_loaded = nothing
	    ema_beta_final_loaded = nothing
	    ema_tau_loaded = nothing
	    ema_step_loaded = nothing
	    if load_enabled && isfile(load_path_final)
	        @warn "Loading InvertibleGame self checkpoint from $load_path_final"
	        loaded_model, meta = load_self(load_path_final)
	        model = loaded_model
	        losses_loaded = meta.losses
	        ema = meta.ema
	        ema_beta_start_loaded = get(meta, :ema_beta_start, nothing)
	        ema_beta_final_loaded = get(meta, :ema_beta_final, nothing)
	        ema_tau_loaded = get(meta, :ema_tau, nothing)
	        ema_step_loaded = get(meta, :ema_step, nothing)
	    end

    opt_state_true = Flux.setup(opt, model)
    opt_state_adv = Flux.setup(opt, model)

    losses = Float32[]
    if losses_loaded isa AbstractVector
        append!(losses, Float32.(losses_loaded))
    end
    last_save = time()

	    if ema === nothing
	        ema = deepcopy(model)
	    end

	    ema_beta_start_f = Float32(ema_beta_start_loaded === nothing ? ema_beta_start : ema_beta_start_loaded)
	    ema_beta_final_f = Float32(ema_beta_final_loaded === nothing ? ema_beta_final : ema_beta_final_loaded)
	    ema_tau_f = Float32(ema_tau_loaded === nothing ? ema_tau : ema_tau_loaded)
	    ema_step = Int(ema_step_loaded === nothing ? 0 : ema_step_loaded)
	    _ = _ema_beta(ema_step, ema_beta_start_f, ema_beta_final_f, ema_tau_f) # validation

	    x_mem = zeros(Float32, model.dim, 0)
	    c_mem = zeros(Float32, model.context_dim, 0)
    mem_filled = false

    for _ in 1:Int(epochs)
        ctx_buf = Any[]
        x_buf = Any[]

        flush_batch!() = begin
            isempty(x_buf) && return nothing

            x_true = Float32.(reduce(hcat, map(_as_vec, x_buf)))
            context = Float32.(reduce(hcat, map(_as_vec, ctx_buf)))
            empty!(x_buf)
            empty!(ctx_buf)

            x_batch = (use_memory && mem_filled) ? hcat(x_true, x_mem) : x_true
            c_batch = (use_memory && mem_filled) ? hcat(context, c_mem) : context

	                if use_memory
		                    grads, loss, extras = gradient(model, ema, x_batch, c_batch;
		                                                   margin_true=margin_true, margin_adv=margin_adv, rng=rng,
		                                                   norm_kind=norm_kind,
		                                                   latent_radius_min=latent_radius_min,
		                                                   w_true=w_true, w_reject=w_reject,
		                                                   mode=grad_mode,
		                                                   return_loss=true, return_true_hinges=true)
	                    x_mem, c_mem, mem_filled = _update_memory(x_mem, c_mem, batch_size, x_batch, c_batch, extras.true_norms)
	                else
		                    grads, loss = gradient(model, ema, x_batch, c_batch;
		                                           margin_true=margin_true, margin_adv=margin_adv, rng=rng,
		                                           norm_kind=norm_kind,
		                                           latent_radius_min=latent_radius_min,
		                                           w_true=w_true, w_reject=w_reject,
		                                           mode=grad_mode,
		                                           return_loss=true)
	                end

            if grad_mode === :sum
                Flux.update!(opt_state_true, model, grads)
            else
                g_true, g_adv = grads
                Flux.update!(opt_state_true, model, g_true)
                Flux.update!(opt_state_adv, model, g_adv)
            end
            β = _ema_beta(ema_step, ema_beta_start_f, ema_beta_final_f, ema_tau_f)
            _ema_update!(ema, model, β)
            ema_step += 1
            push!(losses, Float32(loss))

            if save_enabled && (time() - last_save) >= save_period
                save_self(save_path_final, model;
                          losses=losses,
                          ema=ema,
                          ema_beta_start=ema_beta_start_f,
                          ema_beta_final=ema_beta_final_f,
                          ema_tau=ema_tau_f,
                          ema_step=ema_step)
                last_save = time()
            end
            return nothing
        end

        for item in data_iter
            context, x = _unpack_flow_sample(item)

            if (x isa AbstractMatrix) != (context isa AbstractMatrix)
                throw(ArgumentError("context and sample must both be vectors or both be matrices"))
            end

            if x isa AbstractMatrix
                size(x, 1) == model.dim || throw(DimensionMismatch("sample must have $(model.dim) rows"))
                size(context, 1) == model.context_dim || throw(DimensionMismatch("context must have $(model.context_dim) rows"))
                size(context, 2) == size(x, 2) || throw(DimensionMismatch("context batch must match sample batch"))

                x_true = Float32.(Matrix(x))
                c_batch = Float32.(Matrix(context))
                x_batch = (use_memory && mem_filled) ? hcat(x_true, x_mem) : x_true
                c_comb = (use_memory && mem_filled) ? hcat(c_batch, c_mem) : c_batch

	                if use_memory
		                    grads, loss, extras = gradient(model, ema, x_batch, c_comb;
		                                                   margin_true=margin_true, margin_adv=margin_adv, rng=rng,
		                                                   norm_kind=norm_kind,
		                                                   latent_radius_min=latent_radius_min,
		                                                   w_true=w_true, w_reject=w_reject,
		                                                   mode=grad_mode,
		                                                   return_loss=true, return_true_hinges=true)
	                    x_mem, c_mem, mem_filled = _update_memory(x_mem, c_mem, batch_size, x_batch, c_comb, extras.true_norms)
	                else
		                    grads, loss = gradient(model, ema, x_batch, c_comb;
		                                           margin_true=margin_true, margin_adv=margin_adv, rng=rng,
		                                           norm_kind=norm_kind,
		                                           latent_radius_min=latent_radius_min,
		                                           w_true=w_true, w_reject=w_reject,
		                                           mode=grad_mode,
		                                           return_loss=true)
	                end
	
	                if grad_mode === :sum
	                    Flux.update!(opt_state_true, model, grads)
	                else
	                    g_true, g_adv = grads
	                    Flux.update!(opt_state_true, model, g_true)
	                    Flux.update!(opt_state_adv, model, g_adv)
	                end
	                β = _ema_beta(ema_step, ema_beta_start_f, ema_beta_final_f, ema_tau_f)
	                _ema_update!(ema, model, β)
	                ema_step += 1
	                push!(losses, Float32(loss))

	                if save_enabled && (time() - last_save) >= save_period
                    save_self(save_path_final, model;
                              losses=losses,
                              ema=ema,
                              ema_beta_start=ema_beta_start_f,
                              ema_beta_final=ema_beta_final_f,
                              ema_tau=ema_tau_f,
                              ema_step=ema_step)
	                    last_save = time()
	                end
                continue
            end

            push!(ctx_buf, context)
            push!(x_buf, x)
            if length(x_buf) >= batch_size
                flush_batch!()
            end
        end

        flush_batch!()
    end

	    if save_enabled
	        save_self(save_path_final, model;
	                  losses=losses,
	                  ema=ema,
	                  ema_beta_start=ema_beta_start_f,
	                  ema_beta_final=ema_beta_final_f,
	                  ema_tau=ema_tau_f,
	                  ema_step=ema_step)
	    end

    return (; model, ema, losses)
end

"""
    build(InvertibleCoupling, data_iter; kwargs...) -> (model, ema, losses)

Construct and train a single [`InvertibleCoupling`](@ref) network from a dataset iterator.

`dim` and `context_dim` are inferred from the first element of `data_iter` (assumed re-iterable),
and all other construction arguments are provided as keyword arguments.

# Arguments
- `InvertibleCoupling`: network type.
- `data_iter`: iterable dataset of named tuples `(; context, sample)`.

# Keyword Arguments
- `epochs=1`: passed to [`train!`](@ref). `epochs=0` performs no updates (but can still load/save checkpoints).
- `batch_size=32`: passed to [`train!`](@ref).
- `margin_true=0.5`: passed to [`train!`](@ref).
- `margin_adv=0.0`: passed to [`train!`](@ref).
- `norm_kind=:l1`: passed to [`train!`](@ref).
- `w_true=1.0`: passed to [`train!`](@ref).
- `w_reject=1.0`: passed to [`train!`](@ref).
- `ema_beta_start=0.0`: passed to [`train!`](@ref).
- `ema_beta_final=0.9999`: passed to [`train!`](@ref).
- `ema_tau=10000.0`: passed to [`train!`](@ref).
- `latent_radius_min=0.0`: passed to [`train!`](@ref).
- `use_memory=false`: passed to [`train!`](@ref).
- `opt=Flux.Adam(1f-3)`: passed to [`train!`](@ref).
- `rng=Random.default_rng()`: RNG passed to [`train!`](@ref) / [`gradient`](@ref) for latent sampling.
- `save_path=""`: passed to [`train!`](@ref) (empty disables saving).
- `load_path=save_path`: passed to [`train!`](@ref).
- `save_period=60.0`: passed to [`train!`](@ref).
- `rng_model=Random.default_rng()`: RNG used to initialize the model (permutations/weights).
- `spec=nothing`: passed to `InvertibleCoupling(dim, context_dim; spec=...)`.
- `logscale_clamp=2.0`: passed to the model.

# Returns
`(model, ema, losses)`
"""
	function build(::Type{InvertibleCoupling},
	               data_iter;
	               epochs::Integer=1,
	               batch_size::Integer=32,
	               margin_true::Real=0.5,
	               margin_adv::Real=0.0,
	               norm_kind::Symbol=:l1,
	               w_true::Real=1.0,
	               w_reject::Real=1.0,
	               grad_mode::Symbol=:sum,
	               ema_beta_start::Real=0.0,
	               ema_beta_final::Real=0.9999,
	               ema_tau::Real=10000.0,
	               latent_radius_min::Real=0.0,
	               use_memory::Bool=false,
	               opt=Flux.Adam(1f-3),
	               rng::Random.AbstractRNG=Random.default_rng(),
	               save_path::AbstractString="",
	               load_path::AbstractString=save_path,
	               save_period::Real=60.0,
	               rng_model::Random.AbstractRNG=Random.default_rng(),
	               spec=nothing,
	               logscale_clamp::Real=2.0)
    first_item = iterate(data_iter)
    first_item === nothing && throw(ArgumentError("data_iter is empty"))
    item, _ = first_item
    context0, x0 = _unpack_flow_sample(item)

    if (x0 isa AbstractVector) != (context0 isa AbstractVector)
        throw(ArgumentError("context and sample must both be vectors or both be matrices"))
    end

    dim = x0 isa AbstractVector ? length(x0) : size(x0, 1)
    context_dim = context0 isa AbstractVector ? length(context0) : size(context0, 1)
    model = InvertibleCoupling(dim, context_dim; spec=spec, logscale_clamp=logscale_clamp, rng=rng_model)

	    out = train!(model, data_iter;
	                 epochs=epochs,
	                 batch_size=batch_size,
	                 margin_true=margin_true,
	                 margin_adv=margin_adv,
	                 norm_kind=norm_kind,
	                 w_true=w_true,
	                 w_reject=w_reject,
	                 grad_mode=grad_mode,
	                 ema_beta_start=ema_beta_start,
	                 ema_beta_final=ema_beta_final,
	                 ema_tau=ema_tau,
	                 latent_radius_min=latent_radius_min,
	                 use_memory=use_memory,
	                 opt=opt,
	                 rng=rng,
	                 save_path=save_path,
	                 load_path=load_path,
	                 save_period=save_period)
    return out.model, out.ema, out.losses
end
